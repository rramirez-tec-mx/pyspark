{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python37","display_name":"Python 3.7 with Spark","language":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.7.9","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}},"colab":{"provenance":[],"collapsed_sections":["P8ogD6eTQ5Oh","srS40yA-Q5Oj","HtRjmwrYQ5Ol","PBDER5tKQ5Ol","3ucciX0fQ5Oo","fcfknDB6Q5Op","EjIIXLnlQ5Oq","eCsBM-v7Q5Or","GzBXUlIKQ5Ow","7iVFBO2YQ5Ox","xoTw_QdGQ5O0","fnpvifOPQ5O1"]}},"cells":[{"cell_type":"code","metadata":{"id":"cAWcbJaUQ__l","executionInfo":{"status":"ok","timestamp":1678321413455,"user_tz":360,"elapsed":49374,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}}},"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n","!tar xf spark-3.3.2-bin-hadoop3.tgz\n","!pip install -q findspark"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjGhD6oPRE3k","executionInfo":{"status":"ok","timestamp":1678321569809,"user_tz":360,"elapsed":173,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}}},"source":["#All imports\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wsRQ59fRUpF","executionInfo":{"status":"ok","timestamp":1678321571710,"user_tz":360,"elapsed":759,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}}},"source":["import findspark\n","findspark.init()\n","from google.colab import files\n","from pyspark import SparkContext, SparkConf,SQLContext\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import isnan, when, count, col, lit\n","import pandas as pd \n","import numpy as np\n","import matplotlib.pyplot as plt "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHxty3n1TsTJ","executionInfo":{"status":"ok","timestamp":1678321582539,"user_tz":360,"elapsed":6836,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}}},"source":["# create SparkSession\n","spark = SparkSession \\\n","    .builder \\\n","    .getOrCreate()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHzZFC7u6wR4","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1678321582553,"user_tz":360,"elapsed":77,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}},"outputId":"8232abfb-c496-47e5-d553-6e234ca567b2"},"source":["spark.version"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.3.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"v06mmfN4PmFk","executionInfo":{"status":"ok","timestamp":1678321592755,"user_tz":360,"elapsed":175,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}}},"source":["#And SparkContext\n","#sc = spark._sc\n","sc = spark.sparkContext"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7xSemoV7ANF","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1678321595914,"user_tz":360,"elapsed":210,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}},"outputId":"6b18102c-0f62-4374-de5e-c12ad0cea226"},"source":["sc.version"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.3.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Mf2X_U9ZPlhH"},"source":["#sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MExYLtEGTxbR"},"source":["sqlContext = SQLContext(sc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Yj89iQ-vQ5OZ"},"source":["# <center>Getting Started with PySpark</center>"]},{"cell_type":"markdown","metadata":{"id":"VCa3cMShQ5Ob"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"QnjCLBsUQ5Ob"},"source":["Apache Spark is a fast and powerful framework that provides an API to perform massive distributed processing over resilient sets of data. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. Spark SQL is Apache Spark's module for working with structured data and MLlib is Apache Spark's scalable machine learning library. Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark community released a tool, PySpark. PySpark has similar computation speed and power as Scala. PySpark is a parallel and distributed engine for running big data applications. Using PySpark, you can work with RDDs in Python programming language. \n","\n","This tutorial discusses Big Data via PySpark, a Python package for spark programming. Spark's high level libraries such as SparkSQL, Mllib will be used to interact with two different datasets."]},{"cell_type":"markdown","metadata":{"id":"e4COC6zqQ5Oc"},"source":["## What is SparkContext?"]},{"cell_type":"markdown","metadata":{"id":"qgwimdSgQ5Od"},"source":["Spark comes with interactive python shell in which PySpark is already installed in it. PySpark automatically creates a SparkContext for you in the PySpark Shell. SparkContext is an entry point into the world of Spark. An entry point is a way of connecting to Spark cluster. We can use SparkContext using **sc** variable. In the following examples, we retrieve SparkContext version and Python version of SparkContext."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"_G-_9FgzQ5Od","executionInfo":{"status":"ok","timestamp":1646168799407,"user_tz":360,"elapsed":246,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"b3bc1f98-eb03-4d69-b781-40d46c2df850"},"source":["# to retrieve SparkContext version\n","sc.version"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.4.8'"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"PchF7FA_Q5Og","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1646168799409,"user_tz":360,"elapsed":6,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"6cef163b-5fc8-4d72-b909-4e25bbd77596"},"source":["# to retriece Python version of SparkContext\n","sc.pythonVer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.7'"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"kfnVnbvvQ5Oh"},"source":["## Import Packages"]},{"cell_type":"code","metadata":{"id":"wIzKrq-DQ5Oh"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use(\"seaborn\")\n","from math import sqrt\n","from pyspark.mllib.clustering import KMeans"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P8ogD6eTQ5Oh"},"source":["## Using map and filter methods with Lambda function in Python"]},{"cell_type":"markdown","metadata":{"id":"O6iAxtwbQ5Oi"},"source":["Lambda functions are anonymous functions in Python. Anonymous functions do not bind to any name in runtime and it returns the functions without any name. They are usually used with map and filter methods. Lambda functions create functions to be called later. In the following example, we use lambda function with map and flter methods."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSd45SHxQ5Oi","executionInfo":{"status":"ok","timestamp":1646168800023,"user_tz":360,"elapsed":9,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"f09109c3-c6a5-4ea2-b5d9-2bdc02a35c98"},"source":["my_list = [1,2,3,4,5]\n","squared_my_list = list(map(lambda x: x*x, my_list))\n","squared_my_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hh5MCNnXQ5Oj","executionInfo":{"status":"ok","timestamp":1646168800024,"user_tz":360,"elapsed":8,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"c85d501a-991d-48e4-a8ea-72e41a94572b"},"source":["filtered_my_list = list(filter(lambda x: (x%2 != 0), my_list))\n","filtered_my_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 3, 5]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"tSX0soD6Q5VU"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"srS40yA-Q5Oj"},"source":["## Creating RDD from Object"]},{"cell_type":"markdown","metadata":{"id":"iuMkEBaOQ5Ok"},"source":["RDDs are data stacks distributed throughout cluster of computers. Each stack is calculated on different computers in the cluster. RDDs are the most basic data structure of Spark. We can create RDD by giving existing object like Python list to SparkContext's parallelize method. In the following example, we create a list with numbers, then we create a RDD from this list."]},{"cell_type":"code","metadata":{"id":"_-vQWSSfQ5Ok","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168800024,"user_tz":360,"elapsed":5,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"10039f22-aedb-4e78-cab5-a2636defeca1"},"source":["numbers = list(range(0,10))\n","print(numbers)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"]}]},{"cell_type":"code","metadata":{"id":"1WGwyZ63Q5Ok"},"source":["# load the numbers into PySpark\n","numbersRDD = sc.parallelize(numbers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"roQLysrXQ5Ol","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168800557,"user_tz":360,"elapsed":4,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"287773a0-348b-4045-e272-e336aeed4014"},"source":["print(type(numbersRDD))\n","print(type(numbers))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pyspark.rdd.RDD'>\n","<class 'list'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"HtRjmwrYQ5Ol"},"source":["## Transformations and Actions on RDD"]},{"cell_type":"markdown","metadata":{"id":"bvUR1a-eQ5Ol"},"source":["Transformations and actions are two type of operations in Spark. Transformations create new RDDs. Actions performs computation on the RDDs. Map, filter, flatmap and union are basic RDD transformations. Collect, take, first and count are basic RDD actions. In the following example, we create rdd named numRDD from list and then using map transformation we create a new rdd named cubeRDD from numRDD. Finally, we use collect action to return a list that contains all of the elements in this RDD."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QCSBV8zQ5Ol","executionInfo":{"status":"ok","timestamp":1646168802616,"user_tz":360,"elapsed":2061,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"2b154ecd-a66c-44ca-8288-524dfe08caf4"},"source":["#Create RDD, map() transformations\n","numRDD = sc.parallelize(numbers)\n","cubeRDD = numRDD.map(lambda x: x**3)\n","numbers_all = cubeRDD.collect()\n","\n","[print(num) for num in numbers_all];"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","8\n","27\n","64\n","125\n","216\n","343\n","512\n","729\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xICNv9ERZvwl","executionInfo":{"status":"ok","timestamp":1646168802800,"user_tz":360,"elapsed":185,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"2e242b18-401b-41f4-bf35-9bc3d1a5ad8d"},"source":["#take shows the first n values\n","numRDD.take(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vie--CtScREV","executionInfo":{"status":"ok","timestamp":1646168802971,"user_tz":360,"elapsed":172,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"bba4703e-1ddd-4051-e704-71047d29e4f1"},"source":["#reduce reduces de values using a conmutative and associative binary operator\n","from operator import add\n","numRDD.reduce(add)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["45"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dzw41uMVcRcF","executionInfo":{"status":"ok","timestamp":1646168803133,"user_tz":360,"elapsed":3,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"2f832a0a-60ef-48e8-f4a2-db694a746e66"},"source":["numRDD.stats()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(count: 10, mean: 4.5, stdev: 2.8722813232690143, max: 9.0, min: 0.0)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVcgFi07c1kl","executionInfo":{"status":"ok","timestamp":1646168803478,"user_tz":360,"elapsed":346,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"cb997002-e488-4aa1-b927-b7582db53331"},"source":["numRDD.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"PBDER5tKQ5Ol"},"source":["## Transformations and Actions on pair RDD"]},{"cell_type":"markdown","metadata":{"id":"wUiD_U5pQ5Om"},"source":["Pair RDD is a special type of RDD to work with datasets with key/value pairs. All regular transformations work on pair RDD. In the following example, we create pair RDD with 4 tuple with two numbers. In each tuple, the first number is key and the second number is value. Then, we apply reduceByKey transformation to pair RDD. ReduceByKey tranformation combine values with the same key. Therefore, this transformation adds the values of tuples with the same key."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fTOqStOXQ5Om","executionInfo":{"status":"ok","timestamp":1646168805072,"user_tz":360,"elapsed":1595,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"2befb588-e849-4dbf-ee93-8681c4ab401a"},"source":["# create pairRdd with key and value pairs\n","pairRdd = sc.parallelize([(1,2), (3,4), (3,6), (3,7), (4,5)])\n","\n","[print(num) for num in pairRdd.collect()];\n","\n","print(\"\\n\")\n","\n","pairRdd_Reduced = pairRdd.reduceByKey(lambda x, y: x+y)\n","\n","[print(num) for num in pairRdd_Reduced.collect()];"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 2)\n","(3, 4)\n","(3, 6)\n","(3, 7)\n","(4, 5)\n","\n","\n","(4, 5)\n","(1, 2)\n","(3, 17)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SvpsoDaqealX","executionInfo":{"status":"ok","timestamp":1646168805245,"user_tz":360,"elapsed":174,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"59ed4294-9fe2-4a40-be62-992fd2488780"},"source":["pairRdd.take(6)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 2), (3, 4), (3, 6), (3, 7), (4, 5)]"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"Zp9CLk38Q5Om"},"source":["We can sort keys of tuples using sortByKey transformation like in the following example."]},{"cell_type":"code","metadata":{"id":"63kwcnrDQ5Om","outputId":"1a897ad9-d2a1-4b7e-c63e-aefb3fb27757","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168806066,"user_tz":360,"elapsed":823,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}}},"source":["pairRdd_Reduced_Sort = pairRdd_Reduced.sortByKey(ascending=True)\n","\n","[print(num) for num in pairRdd_Reduced_Sort.collect()];"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 2)\n","(3, 17)\n","(4, 5)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"skqvWg7nlf4U","executionInfo":{"status":"ok","timestamp":1646168806280,"user_tz":360,"elapsed":217,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"1242dbfc-6a15-489c-cea7-89df7db72d90"},"source":["pairRdd.take(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 2), (3, 4), (3, 6)]"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"NXKo2u2BQ5On"},"source":["We can count number of tuples with the same key. In the following example, we see (3,2) because there are two tuple with key 3 in pairRdd."]},{"cell_type":"code","metadata":{"id":"D_-aaeukQ5On"},"source":["total = pairRdd.countByKey()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzmRqo54Q5On","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168806455,"user_tz":360,"elapsed":6,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"315bf868-394a-4f94-be4f-8f59dd580823"},"source":["total.items()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_items([(1, 1), (3, 3), (4, 1)])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"gLJXOFInQ5On","outputId":"d37f00af-5c1e-4028-ac8a-d229181e1035","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168806455,"user_tz":360,"elapsed":4,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}}},"source":["[print(\"key\", k, \"has\", v, \"counts\") for k,v in total.items()];"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["key 1 has 1 counts\n","key 3 has 3 counts\n","key 4 has 1 counts\n"]}]},{"cell_type":"markdown","metadata":{"id":"3ucciX0fQ5Oo"},"source":["## What is SparkSession?"]},{"cell_type":"markdown","metadata":{"id":"wnNSOIfUQ5Oo"},"source":["SparkContext is the main entry point for creating RDDs while SparkSession provides a single point of entry to interact with Spark Dataframes. SparkSession is used to create DataFrame, register DataFrames, execute SQL queries. We can access SparkSession in PySpark using spark variable.\n","In the following examples, we retrieve SparkSession version and other informations about it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"id":"IzwHkmNoQ5Oo","executionInfo":{"status":"ok","timestamp":1646168809041,"user_tz":360,"elapsed":2589,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"76ae49b9-dd6b-4f98-f904-3d23fed16cd8"},"source":["spark"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://11b9222054da:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v2.4.8</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7feedaf4a250>"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"fcfknDB6Q5Op"},"source":["## Creating PySpark DataFrame from RDD"]},{"cell_type":"markdown","metadata":{"id":"BX5q2jXKQ5Op"},"source":["Spark SQL which is a Spark module for structured data processing provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. In the following example, we create rdd from list then we create PySpark dataframe using SparkSession's createDataFrame method. When we look at the type of dataframe, we can see pyspark.sql.dataframe as an output. Furthermore, we can use show method to print out the dataframe."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vewSLEE5Q5Op","executionInfo":{"status":"ok","timestamp":1646168813279,"user_tz":360,"elapsed":4243,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"4d8e4452-a39b-48aa-9530-e8e47e81ea95"},"source":["# Create a PySpark Dataframe from rdd\n","sample_list = [(\"Mona\", 23), (\"Lisa\", 29), ('Leonardo',37), ('Piero',41)]\n","rdd = sc.parallelize(sample_list)\n","df_names = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n","type(df_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_q19h0TPQ5Op","executionInfo":{"status":"ok","timestamp":1646168816016,"user_tz":360,"elapsed":2741,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"9207d6a6-c765-47df-fdcd-a4d093aca519"},"source":["df_names.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+---+\n","|    Name|Age|\n","+--------+---+\n","|    Mona| 23|\n","|    Lisa| 29|\n","|Leonardo| 37|\n","|   Piero| 41|\n","+--------+---+\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZQFWmNLitl7","executionInfo":{"status":"ok","timestamp":1646168816641,"user_tz":360,"elapsed":629,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"95c13e77-34ff-404d-b80d-ae08717e97a9"},"source":["columns=[\"New Name\",\"New Age\"]\n","SDF=rdd.toDF(columns)\n","SDF.show()\n","type(SDF)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+-------+\n","|New Name|New Age|\n","+--------+-------+\n","|    Mona|     23|\n","|    Lisa|     29|\n","|Leonardo|     37|\n","|   Piero|     41|\n","+--------+-------+\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpUqHg1TTkQP","executionInfo":{"status":"ok","timestamp":1646168817262,"user_tz":360,"elapsed":625,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"344aec5e-6b12-470d-d7a8-8a82fbc1e9eb"},"source":["df_names.rdd.take(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(Name='Mona', Age=23),\n"," Row(Name='Lisa', Age=29),\n"," Row(Name='Leonardo', Age=37),\n"," Row(Name='Piero', Age=41)]"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"Dh4EySSHlALM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168817263,"user_tz":360,"elapsed":7,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"bceb2f1f-ae82-44a0-a662-405754860c41"},"source":["df_names.rdd.getNumPartitions()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"EjIIXLnlQ5Oq"},"source":["## Add Datasets"]},{"cell_type":"markdown","metadata":{"id":"rv4bNWeZYPfJ"},"source":["You can delete df_data_1.take(5) part and then copy cos.url('file_name', 'bucket_name') above it then assign cos.url('file_name', 'bucket_name') to path_people variable and comment out this variable. cos.url('file_name', 'bucket_name') is path to your file you can access the file by using this path."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N78PFBjmYOSW","executionInfo":{"status":"ok","timestamp":1646168901097,"user_tz":360,"elapsed":83839,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"a09a55da-7259-4544-d060-fc500cbffac8"},"source":["#Mount Google drive, read local file. The file was previously uploaded to Google Drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mspark-2.4.8-bin-hadoop2.7\u001b[0m/  spark-2.4.8-bin-hadoop2.7.tgz\n"]}]},{"cell_type":"code","metadata":{"id":"E6_6ATXIF3Ni"},"source":["path_people=\"/content/drive/MyDrive/Colab Notebooks/The Learning Gate/people.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8FTugQCiYfR3","executionInfo":{"status":"ok","timestamp":1646168906566,"user_tz":360,"elapsed":5274,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"36f91b4a-9963-479c-d765-780cdc393e28"},"source":["df_data_1 = sqlContext.read.csv(path_people,header=True,inferSchema=True)\n","#df = spark.read.csv(path_csv,header=True,inferSchema=True).coalesce(2)\n","#df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"minPartitions\",2).load(path_csv)\n","df_data_1.show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------+----------------+------+-------------+\n","|_c0|person_id|            name|   sex|date of birth|\n","+---+---------+----------------+------+-------------+\n","|  0|      100|  Penelope Lewis|female|   1990-08-31|\n","|  1|      101|   David Anthony|  male|   1971-10-14|\n","|  2|      102|       Ida Shipp|female|   1962-05-24|\n","|  3|      103|    Joanna Moore|female|   2017-03-10|\n","|  4|      104|  Lisandra Ortiz|female|   2020-08-05|\n","|  5|      105|   David Simmons|  male|   1999-12-30|\n","|  6|      106|   Edward Hudson|  male|   1983-05-09|\n","|  7|      107|    Albert Jones|  male|   1990-09-13|\n","|  8|      108|Leonard Cavender|  male|   1958-08-08|\n","|  9|      109|  Everett Vadala|  male|   2005-05-24|\n","+---+---------+----------------+------+-------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PwvkVDegQ5Oq","executionInfo":{"status":"ok","timestamp":1646168906567,"user_tz":360,"elapsed":9,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"0d31d6a5-0251-4292-a8ca-3b8dd5d0ca3e"},"source":["df_data_1.take(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(_c0=0, person_id=100, name='Penelope Lewis', sex='female', date of birth='1990-08-31'),\n"," Row(_c0=1, person_id=101, name='David Anthony', sex='male', date of birth='1971-10-14'),\n"," Row(_c0=2, person_id=102, name='Ida Shipp', sex='female', date of birth='1962-05-24'),\n"," Row(_c0=3, person_id=103, name='Joanna Moore', sex='female', date of birth='2017-03-10'),\n"," Row(_c0=4, person_id=104, name='Lisandra Ortiz', sex='female', date of birth='2020-08-05')]"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"jHBTFdv-Y78W"},"source":["You can also add 5000_points.txt dataset by applying same procedure but click insert to code then click insert credentials then write \"file\" and \"bucket\" values inside \"path_5000 = cos.url('file_name', 'bucket_name')\" expression and comment out path_5000."]},{"cell_type":"code","metadata":{"id":"-5-XG0jwY4pX"},"source":["path_5000 = \"drive/My Drive/Colab Notebooks/pySpark/Getting Started with PySpark/5000_points.txt\"\n","#df_people = spark.sparkContext.textFile(path_5000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCsBM-v7Q5Or"},"source":["## Create PySpark DataFrame from external file"]},{"cell_type":"markdown","metadata":{"id":"1tRtQd9lQ5Os"},"source":["We can create PySpark DataFrame by using SparkSession's read.csv method. To do this, we should give path of csv file as an argument to the method. Show action prints first 20 rows of DataFrame. Count action prints number of rows in DataFrame. Columns attribute prints the list of columns in DataFrame. PrintSchema action prints the types of columns in the Dataframe and it gives information about whether there is null values in columns or not."]},{"cell_type":"code","metadata":{"id":"vAGLc7iBQ5Os"},"source":["df_people = spark.read.csv(path_people, header=True, inferSchema=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ep3ByLOQ5Os","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168907647,"user_tz":360,"elapsed":202,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"fe8fe70a-19bc-4540-f4c1-88167adab484"},"source":["df_people.show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------+----------------+------+-------------+\n","|_c0|person_id|            name|   sex|date of birth|\n","+---+---------+----------------+------+-------------+\n","|  0|      100|  Penelope Lewis|female|   1990-08-31|\n","|  1|      101|   David Anthony|  male|   1971-10-14|\n","|  2|      102|       Ida Shipp|female|   1962-05-24|\n","|  3|      103|    Joanna Moore|female|   2017-03-10|\n","|  4|      104|  Lisandra Ortiz|female|   2020-08-05|\n","|  5|      105|   David Simmons|  male|   1999-12-30|\n","|  6|      106|   Edward Hudson|  male|   1983-05-09|\n","|  7|      107|    Albert Jones|  male|   1990-09-13|\n","|  8|      108|Leonard Cavender|  male|   1958-08-08|\n","|  9|      109|  Everett Vadala|  male|   2005-05-24|\n","+---+---------+----------------+------+-------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wh9EhFiZQ5Os","executionInfo":{"status":"ok","timestamp":1646168908563,"user_tz":360,"elapsed":918,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"0d8250fd-3bfc-49f0-ca95-6e2a9691bd26"},"source":["df_people.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100000"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80zfiT-_GN1z","executionInfo":{"status":"ok","timestamp":1646168908564,"user_tz":360,"elapsed":6,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"e93193b1-9a99-4168-ddfc-035dfc6d9057"},"source":["print(len(df_people.columns))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnH4_nDCQ5Os","executionInfo":{"status":"ok","timestamp":1646168908564,"user_tz":360,"elapsed":5,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"44ba75f6-fc0c-431e-b41b-203e035f86ed"},"source":["df_people.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['_c0', 'person_id', 'name', 'sex', 'date of birth']"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"0FOQ9uiyQ5Ot","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168908564,"user_tz":360,"elapsed":3,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"7cfc8e90-e72b-4c61-ec8f-d32f4268bd3a"},"source":["df_people.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- _c0: integer (nullable = true)\n"," |-- person_id: integer (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- sex: string (nullable = true)\n"," |-- date of birth: string (nullable = true)\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rB8ThIChI7yG","executionInfo":{"status":"ok","timestamp":1646168908767,"user_tz":360,"elapsed":206,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"a10445db-4573-49e5-c211-120dffd8ff81"},"source":["df_people.drop(\"name\",\"date of birth\").printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- _c0: integer (nullable = true)\n"," |-- person_id: integer (nullable = true)\n"," |-- sex: string (nullable = true)\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"vhTTyOvxQ5Ot"},"source":["We can use select method to select some columns of DataFrame. If we give argument to show method, it prints out rows as number of argument. In the following example it prints out 10 rows. dropDuplicates method removes the duplicate rows of a DataFrame. We can use count action to see how many rows are dropped."]},{"cell_type":"code","metadata":{"id":"5orJw3V_Q5Ot"},"source":["df_people_sub = df_people.select('name', \"sex\", 'date of birth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqRkCN--Q5Ou","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168908977,"user_tz":360,"elapsed":213,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"7d3cd7d8-a270-4b69-bfc0-359738f8ef62"},"source":["df_people_sub.show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------+------+-------------+\n","|            name|   sex|date of birth|\n","+----------------+------+-------------+\n","|  Penelope Lewis|female|   1990-08-31|\n","|   David Anthony|  male|   1971-10-14|\n","|       Ida Shipp|female|   1962-05-24|\n","|    Joanna Moore|female|   2017-03-10|\n","|  Lisandra Ortiz|female|   2020-08-05|\n","|   David Simmons|  male|   1999-12-30|\n","|   Edward Hudson|  male|   1983-05-09|\n","|    Albert Jones|  male|   1990-09-13|\n","|Leonard Cavender|  male|   1958-08-08|\n","|  Everett Vadala|  male|   2005-05-24|\n","+----------------+------+-------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"HLuuboRlQ5Ou"},"source":["df_people_sub_nodup = df_people_sub.dropDuplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jElA3V76Q5Ou","outputId":"4c6fe920-2a37-4aab-9fc1-bd02f0097919","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168914137,"user_tz":360,"elapsed":5162,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}}},"source":["print(\"Before removing duplicates:\", df_people_sub.count())\n","print(\"After removing dublicates:\", df_people_sub_nodup.count())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before removing duplicates: 100000\n","After removing dublicates: 99998\n"]}]},{"cell_type":"markdown","metadata":{"id":"I5Om62oaQ5Ov"},"source":["We can filter out the rows based on a condition by using filter transformation as in the following example."]},{"cell_type":"code","metadata":{"id":"dAnB37bLQ5Ov"},"source":["df_people_female = df_people.filter(df_people.sex == \"female\")\n","df_people_male = df_people.filter(df_people.sex == \"male\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGuRzMmCQ5Ov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168915022,"user_tz":360,"elapsed":708,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"6959c224-4d0c-4afb-f7c5-633b05854ef8"},"source":["df_people_female.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49014"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"t-X14P8AQ5Ov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168915189,"user_tz":360,"elapsed":169,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"855aee37-0161-480b-c2b5-fbfedbd90855"},"source":["df_people_female.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------+-----------------+------+-------------+\n","|_c0|person_id|             name|   sex|date of birth|\n","+---+---------+-----------------+------+-------------+\n","|  0|      100|   Penelope Lewis|female|   1990-08-31|\n","|  2|      102|        Ida Shipp|female|   1962-05-24|\n","|  3|      103|     Joanna Moore|female|   2017-03-10|\n","|  4|      104|   Lisandra Ortiz|female|   2020-08-05|\n","| 11|      111|Annabelle Rosseau|female|   1989-07-13|\n","+---+---------+-----------------+------+-------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"XTOVc5XwQ5Ov"},"source":["We can group columns based on their values by using groupby transformation as in the following example."]},{"cell_type":"code","metadata":{"id":"9FJBBFdTQ5Ov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168917344,"user_tz":360,"elapsed":2157,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"921bc3fc-351d-4163-9336-456dcc33011e"},"source":["df_people_sex = df_people.groupby(\"sex\")\n","df_people_sex.count().show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-----+\n","|   sex|count|\n","+------+-----+\n","|  null| 1920|\n","|female|49014|\n","|  male|49066|\n","+------+-----+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"AO5yjPYbQ5Ov"},"source":["We can sorts the DataFrame based on one or more columns by using orderBy transformation."]},{"cell_type":"code","metadata":{"id":"4cFqDNiKQ5Ov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168918150,"user_tz":360,"elapsed":807,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"c9467b48-a67f-4240-88f7-ceec81604dd3"},"source":["df_people.orderBy(\"date of birth\").show(3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---------+---------------+------+-------------+\n","|  _c0|person_id|           name|   sex|date of birth|\n","+-----+---------+---------------+------+-------------+\n","|57359|    57459|   Sharon Perez|female|   1899-08-28|\n","|62233|    62333|Martina Morison|female|   1901-04-21|\n","|96318|    96418|   Lisa Garrett|female|   1901-05-09|\n","+-----+---------+---------------+------+-------------+\n","only showing top 3 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"fpnFji5rQ5Ow"},"source":["We can rename a column in DataFrame by using withColumnRenamed transformation."]},{"cell_type":"code","metadata":{"id":"oRqiGCwDQ5Ow","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646168918307,"user_tz":360,"elapsed":159,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"b4e50a0a-f657-46e4-ed52-764bdbd3192f"},"source":["df_people = df_people.withColumnRenamed(\"date of birth\", \"birth\")\n","df_people.show(3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------+--------------+------+----------+\n","|_c0|person_id|          name|   sex|     birth|\n","+---+---------+--------------+------+----------+\n","|  0|      100|Penelope Lewis|female|1990-08-31|\n","|  1|      101| David Anthony|  male|1971-10-14|\n","|  2|      102|     Ida Shipp|female|1962-05-24|\n","+---+---------+--------------+------+----------+\n","only showing top 3 rows\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"GzBXUlIKQ5Ow"},"source":["## Using SQL queries with DataFrames by using Spark SQL module"]},{"cell_type":"markdown","metadata":{"id":"R1iqeIQkQ5Ow"},"source":["We can also use SQL queries to achieve the same things with DataFrames. Firstly, we should create temporary table by using createOrReplaceTempView method. We should give the name of temporary table as an argument to the method. Then, we can give any query we want to execute to SparkSession's sql method as an argument. Look at the following example."]},{"cell_type":"code","metadata":{"id":"DhpDrhmoGXzh","executionInfo":{"status":"ok","timestamp":1678321994192,"user_tz":360,"elapsed":14389,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}}},"source":["path_people=\"/content/drive/MyDrive/Colab Notebooks/Certificado Data Science y AI (DSA) Live/people.csv\"\n","df_people = spark.read.csv(path_people, header=True, inferSchema=True)\n","df_people = df_people.withColumnRenamed(\"date of birth\", \"birth\")\n","df_people.createOrReplaceTempView(\"people\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df_people.show(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vSYhTIryX7v","executionInfo":{"status":"ok","timestamp":1678322027813,"user_tz":360,"elapsed":1724,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}},"outputId":"539772de-7008-44cf-d427-e8bdfe1717ed"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------+----------------+------+-------------------+\n","|_c0|person_id|            name|   sex|              birth|\n","+---+---------+----------------+------+-------------------+\n","|  0|      100|  Penelope Lewis|female|1990-08-31 00:00:00|\n","|  1|      101|   David Anthony|  male|1971-10-14 00:00:00|\n","|  2|      102|       Ida Shipp|female|1962-05-24 00:00:00|\n","|  3|      103|    Joanna Moore|female|2017-03-10 00:00:00|\n","|  4|      104|  Lisandra Ortiz|female|2020-08-05 00:00:00|\n","|  5|      105|   David Simmons|  male|1999-12-30 00:00:00|\n","|  6|      106|   Edward Hudson|  male|1983-05-09 00:00:00|\n","|  7|      107|    Albert Jones|  male|1990-09-13 00:00:00|\n","|  8|      108|Leonard Cavender|  male|1958-08-08 00:00:00|\n","|  9|      109|  Everett Vadala|  male|2005-05-24 00:00:00|\n","+---+---------+----------------+------+-------------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"jDyo3cY9KZBL"},"source":["#Not working for some reason\n","\n","query='ALTER TABLE people CHANGE birth date_of_birth'\n","#query='ALTER TABLE people RENAME COLUMN COLUMN birth TO date_of_birth'\n","df_people = spark.sql(query).show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FUisJ0_xLUj","executionInfo":{"status":"ok","timestamp":1678321726322,"user_tz":360,"elapsed":26389,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}},"outputId":"76141c7b-d0e2-433c-947c-0d58842823bf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ye_mUHgHGcQx","executionInfo":{"status":"ok","timestamp":1646168947385,"user_tz":360,"elapsed":305,"user":{"displayName":"Raúl Valente Ramírez Velarde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjsTl_7K9-nt2Nuy6x1vdia7WL7K2k6yyrhPeq3=s64","userId":"12033984646533146741"}},"outputId":"6a1de501-c009-4997-d958-fa246fc81df5"},"source":["query='DESCRIBE people'\n","spark.sql(query).show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---------+-------+\n","| col_name|data_type|comment|\n","+---------+---------+-------+\n","|      _c0|      int|   null|\n","|person_id|      int|   null|\n","|     name|   string|   null|\n","|      sex|   string|   null|\n","|    birth|   string|   null|\n","+---------+---------+-------+\n","\n"]}]},{"cell_type":"code","metadata":{"id":"xjH-RSs7Q5Ow","executionInfo":{"status":"ok","timestamp":1678322116661,"user_tz":360,"elapsed":2017,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"06c044f3-106f-4730-fbba-416b6a5fb032"},"source":["#Use inverted quotes for column names with spaces and triple quote (\"\"\") for query string\n","query=\"\"\"SELECT name, `birth` FROM people WHERE sex==\"male\" ORDER BY `birth`\"\"\"\n","df_people_names = spark.sql(query)\n","df_people_names.show(10)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------+-------------------+\n","|           name|              birth|\n","+---------------+-------------------+\n","|   Tyler Walton|1903-07-14 00:00:00|\n","|  Daniel Naiman|1903-11-07 00:00:00|\n","|   John Merritt|1906-11-04 00:00:00|\n","|   Roger Watkin|1907-12-08 00:00:00|\n","|     Tim Makris|1909-07-11 00:00:00|\n","|    Jeremy Jost|1910-04-14 00:00:00|\n","|  Fredrick Nass|1911-01-12 00:00:00|\n","|     Shaun King|1911-03-27 00:00:00|\n","|Mitchell Martin|1911-07-06 00:00:00|\n","|   Daniel Rutan|1911-08-09 00:00:00|\n","+---------------+-------------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"RaOX-H0yIu7E","executionInfo":{"status":"ok","timestamp":1678322146964,"user_tz":360,"elapsed":2044,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"57dd5042-32f1-47f0-8919-6848b82c4f61"},"source":["query='SELECT name, `birth` FROM people WHERE `birth` BETWEEN \"1903-01-01\" AND \"1911-12-31\" ORDER BY `birth`'\n","df_people_1903_1906 = spark.sql(query)\n","df_people_1903_1906.show(10)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------+-------------------+\n","|           name|              birth|\n","+---------------+-------------------+\n","|   Tyler Walton|1903-07-14 00:00:00|\n","|  Daniel Naiman|1903-11-07 00:00:00|\n","| Christy Dawson|1904-01-11 00:00:00|\n","|   John Merritt|1906-11-04 00:00:00|\n","|   Roger Watkin|1907-12-08 00:00:00|\n","|   Marie Givens|1908-02-15 00:00:00|\n","|Maribel Donahoe|1908-11-27 00:00:00|\n","|    Paula Evans|1909-02-10 00:00:00|\n","|     Tim Makris|1909-07-11 00:00:00|\n","|   Joyce Jacoby|1909-09-13 00:00:00|\n","+---------------+-------------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"VL0AiYLkViqf","executionInfo":{"status":"ok","timestamp":1678322155698,"user_tz":360,"elapsed":2623,"user":{"displayName":"Raúl Valente Ramírez Velarde","userId":"12033984646533146741"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"51347e2f-b2c1-463e-eb24-b29be36c9071"},"source":["query='SELECT sex,COUNT(sex) FROM people WHERE birth BETWEEN \"1903-01-01\" AND \"1911-12-31\" GROUP BY sex'\n","df_people_1903_1906_sex = spark.sql(query)\n","df_people_1903_1906_sex.show()"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+----------+\n","|   sex|count(sex)|\n","+------+----------+\n","|female|         7|\n","|  male|        10|\n","+------+----------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"7iVFBO2YQ5Ox"},"source":["## Create RDD from external file"]},{"cell_type":"markdown","metadata":{"id":"F7ziWSNrQ5Ox"},"source":["The second and mostly used way to create RDD is from external dataset. To do this, we can use SparkContext's textFile method. In the following example, we use 5000_points.txt dataset. To do this, we use path to dataset as an argument to textFile method."]},{"cell_type":"code","metadata":{"id":"QUfJMdnFQ5Ox"},"source":["# load the 5000_points dataset into a rdd named clusterRDD\n","clusterRDD = sc.textFile(path_5000)\n","#NOT clusterRDD.show(10)\n","clusterRDD.take(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xcFFj_LyQ5Ox"},"source":["Default number of partition of rdd is two. We can see that using getNumPartitions method. If we want to increase number of partition, we can give minPartitions argument to textFile method."]},{"cell_type":"code","metadata":{"id":"pmeqUXIQQ5Oy"},"source":["clusterRDD.getNumPartitions()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWOGC1S3Q5Oy"},"source":["clusterRDD = sc.textFile(path_5000,  minPartitions = 5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PL6BHJ3xQ5Oy"},"source":["clusterRDD.getNumPartitions()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDU2aH7wQ5Oy"},"source":["clusterRDD.take(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7UOx5quGQ5Oz"},"source":["We can transform this rdd by splitting the lines based on the tab."]},{"cell_type":"code","metadata":{"id":"8dg3YY9pQ5Oz"},"source":["rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PiKSKKgQ5Oz"},"source":["rdd_split.take(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4gBG93gQ5Oz"},"source":["We can also further transform the splitted RDD to create a list of integers for the two columns."]},{"cell_type":"code","metadata":{"id":"LZuvHp1ZQ5Oz"},"source":["rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5_92g_-Q5Oz"},"source":["rdd_split_int.take(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xoTw_QdGQ5O0"},"source":["## Machine Learning with PySpark MLlib"]},{"cell_type":"markdown","metadata":{"id":"I5kku0vdQ5O0"},"source":["PySpark MLlib is the Apache Spark's scalable machine learning library in Python consisting of common learning algorithms and utilities. We use Kmeans algorithm of MLlib library to cluster data in 5000_points.txt dataset. First, we should define error method to calculate distance from every point to center of its clusters which the points belong to."]},{"cell_type":"code","metadata":{"id":"uC_B5jKDQ5O0"},"source":["def error(point):\n","    center = model.centers[model.predict(point)]\n","    return sqrt(sum([x**2 for x in (point - center)]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwUTqvRXQ5O0"},"source":["We train the model with 4 different number of clusters from 13 to 16 and then calculate the error for all of them. As you see in the output, 16 clusters give minimum error. We retrain the model with the number of cluster with the smallest error. We then use clusterCenters attribute to see the center of all clusters."]},{"cell_type":"code","metadata":{"id":"03_qaHTvQ5O0"},"source":["\n","# Train the model with clusters from 13 to 16 and compute SSE\n","temp = 0\n","cluster = 0\n","for clst in range(13, 17):\n","    model = KMeans.train(rdd_split_int, clst, seed=1)\n","    SSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y) \n","    while temp > SSE or temp == 0:\n","        temp = SSE\n","        cluster = clst\n","    print(\"The cluster\", clst, \"has Sum of Squared Error\", SSE)\n","\n","\n","print(\"\\ncluster\", cluster)\n","print(\"SSE:\", temp)\n","\n","\n","# Train the model again with the best k \n","model = KMeans.train(rdd_split_int, k=cluster, seed=1)\n","\n","# Get cluster centers\n","cluster_centers = model.clusterCenters"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T8E_W9aSQ5O0"},"source":["We can again use SparkSession's createDataFrame method to create DataFrame from rdd. We must convert PySpark DataFrame to Pandas DataFrame in order to visualise data. To do this, we can use toPandas method. We create another Pandas DataFrame from cluster centers list. Then, using matplotlib's scatter method, we can make plot for clusters and their centers."]},{"cell_type":"code","metadata":{"id":"FEjUjUPlQ5O1"},"source":["rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\n","\n","rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n","\n","cluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\n","\n","plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n","plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fnpvifOPQ5O1"},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{"id":"Lgy-Izk6Q5O1"},"source":["This tutorial discusses Big Data via PySpark, a Python package for spark programming. In this tutorial, I explained SparkContext, using map and filter methods with Lambda functions in Python, creating rdd from object and external file, transformations and actions on rdd and pair rdd, SparkSession, creating PySpark Dataframe from rdd and external file, using sql queries with Dataframes by using Spark SQL module and finally, machine learning with PySpark MLlib library."]},{"cell_type":"code","metadata":{"id":"CHbdRzOPQ5O1"},"source":["#Converting between Pandas DF, Spark DF and RDD\n","# load the 5000_points dataset into a rdd named clusterRDD\n","spRDD = sc.textFile(path_5000)\n","#NOT clusterRDD.show(10)\n","spRDD.take(10)\n","# RDD to Spark DataFrame\n","spRDD = spRDD.map(lambda x: x.split(\"\\t\"))\n","spRDD = spRDD.map(lambda x: [int(x[0]), int(x[1])])\n","spRDD.take(10)\n","\n","#Convert to Spark DF\n","sparkDF = spRDD.toDF()\n","sparkDF.show(10)\n","\n","#Convert to Pandas DF\n","pdDF = sparkDF.toPandas()\n","pdDF.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1RZf-j-Q5O2"},"source":["#Converting between Pandas DF, Spark DF and RDD\n","#Converting Pandas DF to Spaek DF\n","spDF_2 = sqlContext.createDataFrame(pdDF)\n","spDF_2.show(10)\n","\n","pdDF.head(10)\n","\n","#Accesing RDD\n","spRDD_2=spDF_2.rdd\n","spRDD_2.take(10)"],"execution_count":null,"outputs":[]}]}